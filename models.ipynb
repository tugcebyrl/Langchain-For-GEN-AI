{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "#LLM--->Metin\n",
    "#Chat Modelleri--->Mesaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm=ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.6,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What is the capital of Turkey\"\n",
    "\n",
    "response=llm.invoke(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PROSER\\anaconda3\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "\n",
    "hf=HuggingFaceHub(\n",
    "    repo_id=\"google/flan-t5-large\",\n",
    "    model_kwargs={\"temperature\":0.7, \"max_length\":100},\n",
    "    huggingfacehub_api_token=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "washington dc\n"
     ]
    }
   ],
   "source": [
    "text = \"What is the capital of USA\"\n",
    "output=hf.invoke(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is the use of machine learning techniques to learn to recognize and interpret patterns in data.\n"
     ]
    }
   ],
   "source": [
    "output=hf.invoke(\"Can you tell me what deep learning is\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Tell me a joke about Elon Musk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages=[HumanMessage(content=\"text\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAI\n",
    "\n",
    "llm=OpenAI()\n",
    "chat_model=ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PROSER\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\PROSER\\anaconda3\\envs\\langchain\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the computer go to the doctor?\n",
      "\n",
      "The computer was not as bad as the person who had the computer. The computer was supposed to be a bad computer, but it was. Of course, there was a case where a computer would give away its key, and it would have only been a bad one. The computer had an important mission and the computer was supposed to help.\n",
      "\n",
      "What was it supposed to do?\n",
      "\n",
      "The computer was supposed to tell us how to proceed with our own lives. The computer had to be\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate\n",
    "\n",
    "hf_model=HuggingFaceHub(\n",
    "    repo_id=\"openai-community/gpt2\",\n",
    "    model_kwargs={\"temperature\":0.7, \"max_length\":50}\n",
    ")\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"punchline\"], \n",
    "    template=\"Why did the computer go to the doctor?\\n{punchline}\"\n",
    ")\n",
    "\n",
    "joke_chain=LLMChain(prompt=template,llm=hf_model)\n",
    "\n",
    "joke=joke_chain.run(punchline=\"\")\n",
    "\n",
    "print(joke)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
